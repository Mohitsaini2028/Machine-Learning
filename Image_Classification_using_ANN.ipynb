{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Classification_using_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohitsaini2028/Machine-Learning/blob/main/Image_Classification_using_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT4a4dwlqhsd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwMdAK1XqmPI"
      },
      "source": [
        "# Tutorial 4: Covid 19 Prediction using Artificial Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9dbM1bxjvnU"
      },
      "source": [
        "Dataset: [Covid 19 Chest X-ray dataset](https://www.kaggle.com/tawsifurrahman/covid19-radiography-database)\n",
        "\n",
        "\n",
        "A team of researchers from Qatar University, Doha, Qatar, and the University of Dhaka, Bangladesh along with their collaborators from Pakistan and Malaysia in collaboration with medical doctors have created a database of chest X-ray images for COVID-19 positive cases along with Normal and Viral Pneumonia images. This COVID-19, normal, and other lung infection dataset is released in stages. In the first release, we have released 219 COVID-19, 1341 normal, and 1345 viral pneumonia chest X-ray (CXR) images. In the first update, we have increased the COVID-19 class to 1200 CXR images. In the 2nd update, we have increased the database to 3616 COVID-19 positive cases along with 10,192 Normal, 6012 Lung Opacity (Non-COVID lung infection), and 1345 Viral Pneumonia images. We will continue to update this database as soon as we have new x-ray images for COVID-19 pneumonia patients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mHnC6nNtBK1"
      },
      "source": [
        "**1. Mount the Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gORvsC4_s_fY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "729226ed-1e87-424d-d9d7-38484da321b2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiZOocimtGeu"
      },
      "source": [
        "**2. Move to the place where data resides**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdbGFIkPlah9"
      },
      "source": [
        "%cd /content/drive/MyDrive/DLA23AUG2021/24AUG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqViJFpyjxJZ"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e8v2RY-tKUs"
      },
      "source": [
        "**3. Unziping the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOWDshT6lb8K"
      },
      "source": [
        "# !unzip covid_dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvuRmzA0tNw0"
      },
      "source": [
        "**4. Install split folder python package**\n",
        "\n",
        "https://pypi.org/project/split-folders/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-4b_r2qlenB"
      },
      "source": [
        "!pip install split_folders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq0KuuUatVi-"
      },
      "source": [
        "**5. Splitting the data in training, testing and validation set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xXGHVOIlheA"
      },
      "source": [
        "# import splitfolders\n",
        "# splitfolders.ratio(\"covid_dataset\", output=\"split\", seed=1337, ratio=(.8, .1, .1), group_prefix=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_rTBEKCtaTR"
      },
      "source": [
        "**6. Loading the dataset with normalization in batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4of-TiQPlkCZ"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Normalize training and validation data in the range of 0 to 1\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Read the training sample and set the batch size \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'split/train/',\n",
        "        target_size=(128, 128),\n",
        "        batch_size=8,\n",
        "        seed=100,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Read Validation data from directory and define target size with batch size\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        'split/val/',\n",
        "        target_size=(128, 128),\n",
        "        batch_size=8,\n",
        "        class_mode='categorical',\n",
        "        seed=1000,\n",
        "        shuffle=False)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        'split/test/',\n",
        "        target_size=(128, 128),\n",
        "        batch_size=8,\n",
        "        seed=500,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqmlFmyitf8O"
      },
      "source": [
        "**7. Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrA7f0eKl4tH"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "inputs = keras.Input(shape=(128, 128,3))\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_437haxhtjFa"
      },
      "source": [
        "**8. Model Compilation and Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DixxjVJelrcg"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "adam = Adam(learning_rate=0.0001)\n",
        "# We are going to use accuracy metrics and cross entropy loss as performance parameters\n",
        "model.compile(adam, loss='categorical_crossentropy', metrics=['acc'])\n",
        "# Train the model \n",
        "history = model.fit(train_generator, \n",
        "      steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "      verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h2kqqa0tpx9"
      },
      "source": [
        "**9. Model saving**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMlZWxBhnfjh"
      },
      "source": [
        "model.save('covid_classification.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v64VrifGtsSn"
      },
      "source": [
        "**10. Model loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5kAXwzBoIzp"
      },
      "source": [
        "from tensorflow.keras import models\n",
        "model = models.load_model('covid_classification.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuGiWfjQtvDq"
      },
      "source": [
        "**11. Model weights saving**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brmKRhGtoZVA"
      },
      "source": [
        "model.save_weights('covid_classification_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_mHfakptxUF"
      },
      "source": [
        "**12. Model weights loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc3YG_vioL9a"
      },
      "source": [
        "model.load_weights('covid_classification_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJhgz0AAtzcm"
      },
      "source": [
        "**13. Plotting accuracy and loss graph for training and validation dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKLbBshSocU6"
      },
      "source": [
        "train_acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfqOqEpVogvN"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epochs = range(len(train_acc)) \n",
        "plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoVhONtnt3sq"
      },
      "source": [
        "**14. Evaluate model performance on test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxXMTffHoiVy"
      },
      "source": [
        "test_output= model.evaluate(test_generator, steps=test_generator.samples/test_generator.batch_size, verbose=1)\n",
        "print(test_output)\n",
        "print(model.metrics_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inzAGua1o1we"
      },
      "source": [
        "References:\n",
        "\n",
        "1. https://pypi.org/project/split-folders/\n",
        "2. https://keras.io/"
      ]
    }
  ]
}